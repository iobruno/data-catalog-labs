{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataHub Emitter for Airbyte Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA with DataHub REST Emiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkspaceConfig(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "    base_url: str\n",
    "\n",
    "class DataHubConfig(BaseModel):\n",
    "    server: str\n",
    "\n",
    "class ConnectionConfig(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "    upstream_datajob: str\n",
    "    downstream_dataset: str\n",
    "\n",
    "class PipelineConfig(BaseModel):\n",
    "    workspace: WorkspaceConfig\n",
    "    environment: str\n",
    "    datahub: DataHubConfig\n",
    "    connections: list[ConnectionConfig] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"./pipeline.yaml\")\n",
    "\n",
    "with open(config_path) as f:\n",
    "    raw = yaml.safe_load(f)\n",
    "\n",
    "config = PipelineConfig.model_validate(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineConfig(workspace=WorkspaceConfig(id='c6171c23-c173-4eff-bdcf-9a1713fa5ca8', name='hackernews_rss_bigquery', base_url='http://localhost:8000'), environment='prod', datahub=DataHubConfig(server='http://localhost:9090'), connections=[ConnectionConfig(id='e37988e6-8ed5-465c-abb2-150639819c62', name='hackernews_rss_front', upstream_datajob='urn:li:dataJob:(urn:li:dataFlow:(airflow,hackernews_rss_bigquery,prod),hackernews_rss_front)', downstream_dataset='urn:li:dataset:(urn:li:dataPlatform:bigquery,iobruno-gcp-labs.hackernews_rss_raw.frontpage_items,PROD)')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DataFlow and DataJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahub.api.entities.datajob.dataflow import DataFlow\n",
    "from datahub.api.entities.datajob.datajob import DataJob\n",
    "from datahub.metadata._urns.urn_defs import DataJobUrn, DatasetUrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_airbyte_url(base_url: str, workspace_id: str, connection_id: str) -> str:\n",
    "    return f\"{base_url}/workspaces/{workspace_id}/connections/{connection_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datajob(config: PipelineConfig, conn: ConnectionConfig, flow: DataFlow) -> DataJob:\n",
    "    job = DataJob(id=conn.id, flow_urn=flow.urn, name=conn.name)\n",
    "    job.upstream_urns.append(DataJobUrn.from_string(conn.upstream_datajob))\n",
    "    job.outlets.append(DatasetUrn.from_string(conn.downstream_dataset))\n",
    "    external_url = fetch_airbyte_url(config.workspace.base_url, config.workspace.id, conn.id)\n",
    "    job.url = external_url\n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = DataFlow(orchestrator=\"airbyte\", id=config.workspace.name, env=config.environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataJob (Airbyte Connection): urn:li:dataJob:(urn:li:dataFlow:(airbyte,hackernews_rss_bigquery,prod),e37988e6-8ed5-465c-abb2-150639819c62)\n",
      "  Name:                 hackernews_rss_front\n",
      "  URL:                  http://localhost:8000/workspaces/c6171c23-c173-4eff-bdcf-9a1713fa5ca8/connections/e37988e6-8ed5-465c-abb2-150639819c62\n",
      "  Upstream (Airflow):   [DataJobUrn(urn:li:dataJob:(urn:li:dataFlow:(airflow,hackernews_rss_bigquery,prod),hackernews_rss_front))]\n",
      "  Downstream (BigQuery):[DatasetUrn(urn:li:dataset:(urn:li:dataPlatform:bigquery,iobruno-gcp-labs.hackernews_rss_raw.frontpage_items,PROD))]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs = [build_datajob(config, conn, flow) for conn in config.connections]\n",
    "\n",
    "for job in jobs:\n",
    "    print(f\"DataJob (Airbyte Connection): {job.urn}\")\n",
    "    print(f\"  Name:                 {job.name}\")\n",
    "    print(f\"  URL:                  {job.url}\")\n",
    "    print(f\"  Upstream (Airflow):   {job.upstream_urns}\")\n",
    "    print(f\"  Downstream (BigQuery):{job.outlets}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Emit MCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datahub.emitter.rest_emitter import DataHubRestEmitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow MCPs: 4\n",
      "Jobs MCPs: 6\n",
      "Total MCPs: 10\n"
     ]
    }
   ],
   "source": [
    "flow_mcps = list(flow.generate_mcp())\n",
    "jobs_mcps = [mcp for job in jobs for mcp in job.generate_mcp()]\n",
    "all_mcps = flow_mcps + jobs_mcps\n",
    "\n",
    "print(f\"Flow MCPs: {len(flow_mcps)}\")\n",
    "print(f\"Jobs MCPs: {len(jobs_mcps)}\")\n",
    "print(f\"Total MCPs: {len(all_mcps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitted 10 MCPs to http://localhost:9090\n"
     ]
    }
   ],
   "source": [
    "emitter = DataHubRestEmitter(gms_server=config.datahub.server)\n",
    "emitter.emit_mcps(all_mcps)\n",
    "\n",
    "print(f\"Emitted {len(all_mcps)} MCPs to {config.datahub.server}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
