source:
  type: airbyte
  config:
    server_url: "${AIRBYTE_SERVER_URL:-http://host.docker.internal:8000/api/public/v1}"
    api_token: "${AIRBYTE_API_TOKEN}"
    workspace_id: "${AIRBYTE_WORKSPACE_ID}"
    platform_instance: "airbyte"
    ingest_connections: true
    ingest_sources: true
    ingest_destinations: true
    ingest_jobs: true

    # BigQuery environment for dataset URNs (default: PROD)
    bigquery_env: "PROD"

    # Optional: Airflow task linkage (maps Airbyte connection_id -> Airflow task_id)
    airflow_dag_id: "hackernews_rss_bigquery"
    airflow_cluster: "prod"
    airflow_connection_mapping:
      "a69f80b8-53f0-4f3c-b742-f7d064683fb9": "hackernews_rss_front"
      "43e6123d-29b1-4172-9ab9-9430102dbf6a": "hackernews_rss_comments"
      "e7a66c6d-60bb-4ee0-8cf7-f88dad6dff29": "hackernews_rss_newest"

sink:
  type: "datahub-kafka"
  config:
    connection:
      bootstrap: "host.docker.internal:9093"
      schema_registry_url: "http://host.docker.internal:8081"

  # Alternatively, you can use DataHub REST Server
  # type: datahub-rest
  # config:
  #   server: "http://host.docker.internal:9090"
