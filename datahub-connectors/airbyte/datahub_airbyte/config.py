from datahub.configuration.common import ConfigModel


class AirbyteConnectionSourceConfig(ConfigModel):
    """Configuration for a single Airbyte connection to register in DataHub.

    Each field maps to a ``${ENV_VAR}`` placeholder in the recipe YAML so that
    the same recipe can be reused across connections by swapping env vars.

    Attributes:
        airflow_dag: Name of the Airflow DAG that triggers the Airbyte sync.
            Used to build both the Airbyte DataFlow URN and the upstream
            Airflow DataJob URN.  Example: ``"hackernews_rss_bigquery"``.
        airflow_task: Task ID inside the Airflow DAG that triggers the sync.
            Used to build the upstream Airflow DataJob URN so that lineage
            connects the Airflow task to the Airbyte connection.
            Example: ``"hackernews_rss_front"``.
        bigquery_table: Fully-qualified BigQuery table produced by the sync,
            in ``project.dataset.table`` format.  Becomes the downstream
            outlet of the Airbyte DataJob.
            Example: ``"iobruno-gcp-labs.hackernews_rss_raw.frontpage_items"``.
        airbyte_connection_id: UUID of the Airbyte connection.  Used as the
            DataJob ID so it is unique and traceable back to Airbyte.
            Example: ``"e37988e6-8ed5-465c-abb2-150639819c62"``.
        environment: DataHub environment label (``"prod"``, ``"dev"``, etc.).
            Defaults to ``"prod"``.  Appears in every URN generated by
            this source.

    Example recipe snippet::

        source:
          type: "datahub_airbyte.source.AirbyteConnectionSource"
          config:
            airflow_dag: ${AIRFLOW_DAG_NAME}
            airflow_task: ${AIRFLOW_TASK_NAME}
            bigquery_table: ${BIGQUERY_TABLE_FQN}
            airbyte_connection_id: ${AIRBYTE_CONNECTION_ID}
            environment: "prod"
    """

    airflow_dag: str
    airflow_task: str
    bigquery_table: str
    airbyte_connection_id: str
    environment: str = "prod"
